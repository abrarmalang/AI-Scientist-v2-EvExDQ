"""
Setup Comparator

Compare experimental setups generated by different approaches.
"""

from typing import Dict, Any, List, Set
import json


class SetupComparator:
    """
    Compare experimental setups (non-agentic vs agentic, or vs ground truth).

    Provides:
    - Quality score comparison
    - Cost/time comparison
    - Component overlap analysis (Jaccard similarity)
    - Winner determination
    """

    def compare(
        self,
        setup_a: Dict[str, Any],
        setup_b: Dict[str, Any],
        labels: tuple = ("Setup A", "Setup B"),
        ground_truth: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Compare two experimental setups.

        Args:
            setup_a: First setup (with metadata)
            setup_b: Second setup (with metadata)
            labels: Labels for setups (e.g., ("non_agentic", "agentic"))
            ground_truth: Optional ground truth for alignment comparison

        Returns:
            Comprehensive comparison dictionary
        """
        label_a, label_b = labels

        # Extract setups and metadata
        actual_setup_a = setup_a.get("setup", setup_a)
        actual_setup_b = setup_b.get("setup", setup_b)
        metadata_a = setup_a.get("metadata", {})
        metadata_b = setup_b.get("metadata", {})

        # Extract nested ai_setup if present
        if "ai_setup" in actual_setup_a:
            actual_setup_a = actual_setup_a["ai_setup"]
        if "ai_setup" in actual_setup_b:
            actual_setup_b = actual_setup_b["ai_setup"]

        comparison = {
            "labels": {
                "setup_a": label_a,
                "setup_b": label_b
            },
            "quality_scores": self._compare_quality_scores(
                metadata_a, metadata_b, label_a, label_b
            ),
            "cost_comparison": self._compare_costs(
                metadata_a, metadata_b, label_a, label_b
            ),
            "component_comparison": {
                "baselines": self._compare_baselines(
                    actual_setup_a, actual_setup_b, label_a, label_b
                ),
                "metrics": self._compare_metrics(
                    actual_setup_a, actual_setup_b, label_a, label_b
                ),
                "datasets": self._compare_datasets(
                    actual_setup_a, actual_setup_b, label_a, label_b
                )
            },
            "summary": {}
        }

        # Add ground truth comparison if available
        if ground_truth:
            comparison["ground_truth_alignment"] = self._compare_with_ground_truth(
                actual_setup_a, actual_setup_b, ground_truth, label_a, label_b
            )

        # Generate summary and recommendation
        comparison["summary"] = self._generate_summary(comparison, label_a, label_b)

        return comparison

    def _compare_quality_scores(
        self,
        metadata_a: Dict,
        metadata_b: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare quality scores from metadata."""
        score_a = metadata_a.get("best_score") or metadata_a.get("quality_score", 0.0)
        score_b = metadata_b.get("best_score") or metadata_b.get("quality_score", 0.0)

        return {
            label_a: score_a,
            label_b: score_b,
            "difference": score_b - score_a,
            "winner": label_b if score_b > score_a else (label_a if score_a > score_b else "tie")
        }

    def _compare_costs(
        self,
        metadata_a: Dict,
        metadata_b: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare computational costs."""
        time_a = metadata_a.get("time_elapsed", 0.0)
        time_b = metadata_b.get("time_elapsed", 0.0)
        calls_a = metadata_a.get("llm_calls", 1)
        calls_b = metadata_b.get("llm_calls", 1)

        return {
            "time": {
                label_a: time_a,
                label_b: time_b,
                "ratio": time_b / time_a if time_a > 0 else float('inf'),
                "difference": time_b - time_a
            },
            "llm_calls": {
                label_a: calls_a,
                label_b: calls_b,
                "ratio": calls_b / calls_a if calls_a > 0 else float('inf'),
                "difference": calls_b - calls_a
            }
        }

    def _compare_baselines(
        self,
        setup_a: Dict,
        setup_b: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare baseline selections."""
        baselines_a = setup_a.get("baselines", [])
        baselines_b = setup_b.get("baselines", [])

        names_a = {self._normalize_name(b.get("name", "")) for b in baselines_a}
        names_b = {self._normalize_name(b.get("name", "")) for b in baselines_b}

        overlap = names_a & names_b
        jaccard = len(overlap) / len(names_a | names_b) if (names_a | names_b) else 0.0

        return {
            "count": {
                label_a: len(baselines_a),
                label_b: len(baselines_b)
            },
            "overlap": {
                "common": list(overlap),
                "count": len(overlap),
                "jaccard_similarity": jaccard
            },
            "unique_to_a": list(names_a - names_b),
            "unique_to_b": list(names_b - names_a),
            "details": {
                label_a: [b.get("name", "") for b in baselines_a],
                label_b: [b.get("name", "") for b in baselines_b]
            }
        }

    def _compare_metrics(
        self,
        setup_a: Dict,
        setup_b: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare metric selections."""
        metrics_a = setup_a.get("metrics", [])
        metrics_b = setup_b.get("metrics", [])

        names_a = {self._normalize_name(m.get("name", "")) for m in metrics_a}
        names_b = {self._normalize_name(m.get("name", "")) for m in metrics_b}

        overlap = names_a & names_b
        jaccard = len(overlap) / len(names_a | names_b) if (names_a | names_b) else 0.0

        return {
            "count": {
                label_a: len(metrics_a),
                label_b: len(metrics_b)
            },
            "overlap": {
                "common": list(overlap),
                "count": len(overlap),
                "jaccard_similarity": jaccard
            },
            "unique_to_a": list(names_a - names_b),
            "unique_to_b": list(names_b - names_a),
            "details": {
                label_a: [m.get("name", "") for m in metrics_a],
                label_b: [m.get("name", "") for m in metrics_b]
            }
        }

    def _compare_datasets(
        self,
        setup_a: Dict,
        setup_b: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare dataset selections."""
        datasets_a = setup_a.get("datasets", [])
        datasets_b = setup_b.get("datasets", [])

        names_a = {self._normalize_name(d.get("name", "")) for d in datasets_a}
        names_b = {self._normalize_name(d.get("name", "")) for d in datasets_b}

        overlap = names_a & names_b
        jaccard = len(overlap) / len(names_a | names_b) if (names_a | names_b) else 0.0

        return {
            "count": {
                label_a: len(datasets_a),
                label_b: len(datasets_b)
            },
            "overlap": {
                "common": list(overlap),
                "count": len(overlap),
                "jaccard_similarity": jaccard
            },
            "unique_to_a": list(names_a - names_b),
            "unique_to_b": list(names_b - names_a),
            "details": {
                label_a: [d.get("name", "") for d in datasets_a],
                label_b: [d.get("name", "") for d in datasets_b]
            }
        }

    def _compare_with_ground_truth(
        self,
        setup_a: Dict,
        setup_b: Dict,
        ground_truth: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Compare both setups against ground truth."""
        # Extract ground truth baselines/metrics/datasets
        gt_baselines = self._extract_ground_truth_baselines(ground_truth)
        gt_metrics = self._extract_ground_truth_metrics(ground_truth)
        gt_datasets = self._extract_ground_truth_datasets(ground_truth)

        # Compare each setup with ground truth
        def compare_with_gt(setup: Dict) -> Dict:
            setup_baselines = {self._normalize_name(b.get("name", "")) for b in setup.get("baselines", [])}
            setup_metrics = {self._normalize_name(m.get("name", "")) for m in setup.get("metrics", [])}
            setup_datasets = {self._normalize_name(d.get("name", "")) for d in setup.get("datasets", [])}

            baseline_overlap = len(setup_baselines & gt_baselines) / len(gt_baselines) if gt_baselines else 0.0
            metric_overlap = len(setup_metrics & gt_metrics) / len(gt_metrics) if gt_metrics else 0.0
            dataset_overlap = len(setup_datasets & gt_datasets) / len(gt_datasets) if gt_datasets else 0.0

            # Weighted average (baselines: 40%, metrics: 40%, datasets: 20%)
            alignment = 0.4 * baseline_overlap + 0.4 * metric_overlap + 0.2 * dataset_overlap

            return {
                "baseline_overlap": baseline_overlap,
                "metric_overlap": metric_overlap,
                "dataset_overlap": dataset_overlap,
                "overall_alignment": alignment
            }

        alignment_a = compare_with_gt(setup_a)
        alignment_b = compare_with_gt(setup_b)

        return {
            label_a: alignment_a,
            label_b: alignment_b,
            "winner": label_b if alignment_b["overall_alignment"] > alignment_a["overall_alignment"]
                     else (label_a if alignment_a["overall_alignment"] > alignment_b["overall_alignment"] else "tie")
        }

    def _extract_ground_truth_baselines(self, ground_truth: Dict) -> Set[str]:
        """Extract baseline names from ground truth."""
        # Try to extract from redacted sections
        experimental_content = ground_truth.get("redacted_sections", {}).get("raw_experimental_content", "")
        # Basic extraction (can be improved)
        return set()  # Placeholder - implement based on ground truth structure

    def _extract_ground_truth_metrics(self, ground_truth: Dict) -> Set[str]:
        """Extract metric names from ground truth."""
        return set()  # Placeholder

    def _extract_ground_truth_datasets(self, ground_truth: Dict) -> Set[str]:
        """Extract dataset names from ground truth."""
        return set()  # Placeholder

    def _normalize_name(self, name: str) -> str:
        """Normalize names for comparison."""
        return name.lower().strip()

    def _generate_summary(
        self,
        comparison: Dict,
        label_a: str,
        label_b: str
    ) -> Dict:
        """Generate summary and recommendation."""
        quality_winner = comparison["quality_scores"]["winner"]
        quality_diff = abs(comparison["quality_scores"]["difference"])

        time_a = comparison["cost_comparison"]["time"][label_a]
        time_b = comparison["cost_comparison"]["time"][label_b]
        time_ratio = comparison["cost_comparison"]["time"]["ratio"]

        calls_ratio = comparison["cost_comparison"]["llm_calls"]["ratio"]

        # Determine overall winner based on quality vs cost
        if quality_winner == "tie":
            overall_winner = label_a if time_a < time_b else label_b
            reason = f"Quality is tied, {overall_winner} is faster"
        elif quality_diff < 0.05:  # Small quality difference
            overall_winner = label_a if time_a < time_b else label_b
            reason = f"Quality difference small (<0.05), {overall_winner} is more efficient"
        else:  # Significant quality difference
            overall_winner = quality_winner
            reason = f"{quality_winner} has significantly better quality (+{quality_diff:.3f})"

        return {
            "quality_winner": quality_winner,
            "cost_winner": label_a if time_a < time_b else label_b,
            "overall_winner": overall_winner,
            "recommendation": reason,
            "key_insights": [
                f"{quality_winner} produced higher quality setup" if quality_winner != "tie" else "Both setups have similar quality",
                f"{label_b} took {time_ratio:.1f}x more time than {label_a}",
                f"{label_b} used {calls_ratio:.1f}x more LLM calls than {label_a}",
                f"Overall: {reason}"
            ]
        }


def format_comparison_report(comparison: Dict) -> str:
    """Format comparison as readable text report."""
    label_a = comparison["labels"]["setup_a"]
    label_b = comparison["labels"]["setup_b"]

    report = f"""
{'='*70}
EXPERIMENTAL SETUP COMPARISON
{'='*70}

{label_a.upper()} vs {label_b.upper()}

{'='*70}
QUALITY SCORES
{'='*70}
  {label_a:20s}: {comparison['quality_scores'][label_a]:.3f}
  {label_b:20s}: {comparison['quality_scores'][label_b]:.3f}
  Difference:          {comparison['quality_scores']['difference']:+.3f}
  Winner:              {comparison['quality_scores']['winner']}

{'='*70}
COMPUTATIONAL COST
{'='*70}
  Time:
    {label_a:20s}: {comparison['cost_comparison']['time'][label_a]:.1f}s
    {label_b:20s}: {comparison['cost_comparison']['time'][label_b]:.1f}s
    Ratio:               {comparison['cost_comparison']['time']['ratio']:.1f}x

  LLM Calls:
    {label_a:20s}: {comparison['cost_comparison']['llm_calls'][label_a]}
    {label_b:20s}: {comparison['cost_comparison']['llm_calls'][label_b]}
    Ratio:               {comparison['cost_comparison']['llm_calls']['ratio']:.1f}x

{'='*70}
COMPONENT OVERLAP
{'='*70}
  Baselines:
    {label_a:20s}: {comparison['component_comparison']['baselines']['count'][label_a]} baselines
    {label_b:20s}: {comparison['component_comparison']['baselines']['count'][label_b]} baselines
    Common:              {comparison['component_comparison']['baselines']['overlap']['count']}
    Jaccard Similarity:  {comparison['component_comparison']['baselines']['overlap']['jaccard_similarity']:.3f}

  Metrics:
    {label_a:20s}: {comparison['component_comparison']['metrics']['count'][label_a]} metrics
    {label_b:20s}: {comparison['component_comparison']['metrics']['count'][label_b]} metrics
    Common:              {comparison['component_comparison']['metrics']['overlap']['count']}
    Jaccard Similarity:  {comparison['component_comparison']['metrics']['overlap']['jaccard_similarity']:.3f}

  Datasets:
    {label_a:20s}: {comparison['component_comparison']['datasets']['count'][label_a]} datasets
    {label_b:20s}: {comparison['component_comparison']['datasets']['count'][label_b]} datasets
    Common:              {comparison['component_comparison']['datasets']['overlap']['count']}
    Jaccard Similarity:  {comparison['component_comparison']['datasets']['overlap']['jaccard_similarity']:.3f}

{'='*70}
SUMMARY
{'='*70}
  Overall Winner:      {comparison['summary']['overall_winner']}
  Recommendation:      {comparison['summary']['recommendation']}

Key Insights:
"""

    for insight in comparison['summary']['key_insights']:
        report += f"  â€¢ {insight}\n"

    report += f"\n{'='*70}\n"

    return report
